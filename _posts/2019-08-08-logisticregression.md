---
layout: post
title: Logistic regression
excerpt: "Logistic regression"
categories: [machine learning]
comments: true
---

이번 글은 [딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/37406) 의 ``로지스틱 회귀`` 글을 참고하여 작성하였습니다.
> [딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/37406)

## Binary Classifiaction

선형회귀 챕터에서 집크기와 집값의 관계를 직선의 방정식으로 표현한다는 가설 하에 주어진 데이터로부터 가중치 W와 편향 b를 찾아 데이터를 가장 잘 표현하는 직선을 찾았습니다. 그런데 이번에 배울 둘 중 하나의 선택지 중에서 정답을 고르는 이진 분류 문제는 직선으로 표현하는 것이 적절하지 않습니다.

예를 들어 아래 표와같이 여러 학생의 시험 성적에 따른 합격 결과 데이터를 가정해 봅시다. 시험성적이 x라면 시험결과는  y 입니다. 이 데이터로부터 특정 점수를 얻었을 때의 합격, 불합격 여부를 알기 위해서는 어떻게 해야 할까요?

![graph]({{ site.url }}/img/logistic-1.PNG)

왼쪽은 데이터이고 오른쪽은 데이터를 그래프로 그려본 것입니다. ``합격 결과``는 1, ``불합격``은 0으로 두었습니다. 두가지 값에 대해 다른 숫자를 사용해도 가능은 하지만 이진분류 에서는 보편적으로 0과 1 을 사용합니다. 그래프에서 보듯이 이진분류에서 그래프는 알파벳 S 자 형태로 표현됩니다. 직선을 사용 할 경우 이 데이터들을 제대로 나타내지 못해 분류작업이 제대로 동작하지 않는 경우가 많습니다.

또한 이진분류에서는 실제값 y가 보통 0 또는 1이라는 두가지 값만을 가집니다. 이 문제를 풀기 위해서 예측값 $$\hat{y}$$ 는 0에서 1 사이의 값을 가지도록 하는 것이 보편적입니다. 왜냐하면 0과 1사이의 값은 확률로 해석할 수 있기 때문입니다. 하지만 선형 회귀의 경우 y값이 음의 무한대부터 양의 무한대와 같은 큰 수들도 가질 수 있는데 이 또한 직선이 분류 문제에 적합하지 않은 이유라 할 수 있습니다.

자 이제 0과 1 사이의 값을 가지면서 S자 형태로 그려지는 조건을 충족하는 함수를 찾아야 합니다. 사실 이 조건들을 충족하는 유명한 함수가 있는데 바로 ``시그모이드````(Sigmoid)`` 함수입니다. 시그모이드는 딥러닝에서 아주 중요한 함수이기 때문에 이를 제대로 이해하고 가는 것이 좋습니다.

## Sigmoid function

우선 시그모이드 함수의 방정식은 아래와 같습니다. 종종 $$\sigma$$로 표현하기도 합니다.

* $$ H(x) = \frac{1}{1+e^{-(Wx+b)}} = sigmoid(Wx+b) = \sigma(Wx+b) $$

![graph]({{ site.url }}/img/logistic-2.PNG)

여기서 ``e``(e=2.718281)는 자연상수 입니다. 위의 시그모이드 그래프에서 보는것과 같이 x가 ``0``일때는 ``0.5``를 가지고 x가 ``증가하면 1``에 수렴, x가 ``감소하면 0``에 수렴합니다. 여기서 구해야 할 것은 여전히 주어진 데이터에 가장 적합한 가중치 W와 편향 bias입니다. 모든 인공지능 알고리즘이 하는 일은 결국 주어진 데이터에 대해 적합한 가중치 W와 b를 구하는 것입니다. 가중치 W와 편향 b의 값에 따라 그래프가 어떻게 변하는지 알아보도록 하겠습니다. 

![graph]({{ site.url }}/img/logistic-3.PNG)

먼저 ``W`` 값이 1일때 초록색, 0.5일때 빨간색, 2일때 초록색 선으로 표현하였습니다. W값에 따라 그래프의 경사도가 변하는 것을 볼 수 있습니다. W 값이 커지면 경사가 커지고 W 값이 작아지면 경사가 작아집니다. 이제  b 값에 따라 그래프가 어떻게 변하는지 확인해보도록 하겠습니다.

![graph]({{ site.url }}/img/logistic-4.PNG)

다음으로 ``b`` 값이 증가했을 때 빨간색, 감소했을 때 초록색 선으로 표현하였습니다. 편향 b 값이 증가하면 그래프가 왼쪽으로 움직이고 감소하면 오른쪽으로 움직이는 것을 볼 수 있습니다.

시그모이드 함수는 입력값이 커지면 1에 수렴하고 입력값이 작아지면 0에 수렴합니다. 0부터 1까지의 값을 가지는데 출력값이 0.5 이상이면 1(True), 0.5 이하이면 0 (False)로 만들어 이진분류 문제에 사용할 수 있습니다.

## Cost function

지난 시간에 선형회귀 분석(Linear regression) 에서 경사하강법을 통해 가중치 W를 업데이트 하는 방법을 배웠습니다. 그 때 사용한 손실함수는 평균제곱오차(MSE) 였습니다. 하지만 이번 이진분류 문제에서는 평균제곱오차를 사용하지 않습니다. 그 이유는 시그모이드 함수에 비용 함수를 평균 제곱 오차로 하여 그래프를 그리면 다음과 같이 되기 때문입니다.

그림에서처럼 로지스틱 회귀에서 평균제곱오차를 비용함수로 사용하면 경사하강법을 사용했을때 잘못하면 local minimum 에 빠질 가능성이 있습니다. 이렇게 되면 cost가 최소가 되는 가중치 W를 찾아야 하는 우리의 목적에 맞지 않죠. 따라서 가중치 W를 최소로 만드는 새로운 손실함수가 필요합니다. 가중치를 최소화하는 아래의 어떤 함수를 목적 함수라고 합시다. 이 책에서는 비용 함수와 목적 함수를 최적의 가중치를 찾기 위해 함수의 값을 최소화하는 함수라는 의미에서 같은 의미의 용어로 사용합니다. $$J$$는 목적 함수(objective function)를 의미합니다

* $$J(W) = \frac{1}{n}\sum{i}{n}f(H(x_i),y_i)$$

위에 식에서 n은 샘플데이터의 개수, $$y_i$$는 실제값, H(x_i)는 예측값( $$\hat{y}$$ 라고도 표현합니다.) 입니다. 위의 식은 어떤 함수가 실제값과 예측값 사이의 오차를 나타내는 식이고 여기서 함수 $$f$$를 어떻게 정의하느냐에 따라서 가중치를 최소화하는 적절한 손실 함수가 완성됩니다. 목적 함수는 전체 데이터에 대해서 어떤 함수 $$f$$의 값의 평균을 계산하고 있습니다. 적절한 가중치를 찾기 위해서는 결과적으로 실제값과 예측값에 대한 오차를 줄여야 하므로 여기서 이 $$f$$는 비용 함수(cost function)라고 하겠습니다. 그럼 식을 다시 아래와 같이 나타낼 수 있습니다.

* $$J(W) = \frac{1}{n}\sum{i}{n}f(H(x_i),y_i)$$

시그모이드 함수를 생각해보면 0과 1사이의 예측값 $$\hat{y}$$를 반환합니다. 그래서 실제값 y가 ``0``이라면 $$\hat{y}$$가 ``1``에 가까우면 오차가 커지고 실제값 y가 ``1``일때는 $$\hat{y}$$가 ``0``에 가까울수록 오차가 커지게 됩니다. 이러한 특성을 로그함수를 이용하여 나타낼 수 있습니다.

실제값이 1일 때의 그래프를 파란색 선으로 표현하였으며, 실제값이 0일 때의 그래프를 빨간색 선으로 표현하였습니다. 위의 그래프를 간략히 설명하면, 실제값 y가 1일 때, 예측값 H(X)가 1이 되면 오차가 0으로 줄어들고 H(x)가 0이 되면 오차가 무한대로 발산합니다. 실제값이 0인 경우는 그 반대로 이해하면 됩니다. 

이때 로지스틱 회귀에서 찾아낸 비용 함수를 크로스 엔트로피(Cross Entropy)함수라고 합니다. 즉, 결론적으로 로지스틱 회귀는 비용 함수로 크로스 엔트로피 함수를 사용하며, 가중치를 찾기 위해서 크로스 엔트로피 함수의 평균을 취한 함수를 사용합니다. 
