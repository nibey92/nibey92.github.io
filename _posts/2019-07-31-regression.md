---
layout: post
title: Regression
excerpt: "Linear regression"
categories: [machine learning]
comments: true
---

이 포스팅은 `KMOOC 자율주행을위한 머신러닝 강좌`를 청강한 후 정리하였습니다.
{: .notice}

딥러닝을 이해하기 위해서는 Linear Regression과 Logistic Regression을 이해할 필요가 있습니다. 오늘은 우선 선형회귀에 대해서 알아보겠습니다. 선형회귀 개념 자체에 대한 이해도 중요하지만 머신러닝에서 쓰이는 용어인 가설, 손실함수, 경사하강법에 대한 개념까지 함께 이해하도록 하겠습니다.

## Linear regression

시험 공부 시간이 늘어날수록 성적이 잘 나오고 운동시간을 늘릴수록 몸무게는 줄어듭니다. 집의 평수가 클수록 집의 매매 가격은 비싼 경향이 있습니다. 수학적으로 생각해보면 어떤 요인의 수치가 특정 수치에 영향을 주고있습니다. 다른 변수의 값을 변하게 하는 변수를 ``x``, 변수 x에 의해 값이 종속적으로 변하는 변수를 ``y``라고 합니다. 

선형 회귀는 종속 변수 y와 한 개 이상의 독립 변수 x 와의 선형 관계를 모델링하는 분석 기법입니다. 즉 x는 1개일수도, 그 이상일 수도 있습니다. 독립변수 x가 1개일 때 단순 선형 회귀라고 합니다.


### Simple Linear Regression Analysis



집의 크기에 따른 집값을 생각 해 봅시다. 집의 크기를 x, 집값이 y라 하면 수식은 다음과 같이 나타낼 수 있습니다.

* $$y = Wx + b$$ 

위의 수식은 단순선형회귀(Simple Linear Regression Analysis)의 수식을 보여줍니다. 여기서 W를 ``가중치(weight)``, b를 ``편향(bias)`` 이라고 합니다.

### Multiple Linear Regression Analysis

* $$ y = W_1x_1 + W_2x_2 + .... W_nx_n +b $$

잘 생각해보면 집값은 집의 크기 뿐만이 아니라 집의 층수, 집이 지어진 연도, 역과의 거리 등의 요소들도 영향을 미칩니다. 이렇게 다수의 요소를 가지고 집의 가격을 예측해 봅시다. y는 여전히 1개이지만 x는 이제 여러개가 되었습니다. 이를 다중 선형 회귀 분석(Multiple Linear Regression Analysis)이라고 합니다.

### feature, sample

위에서 보았던 단순선형회귀 분석과 다중선형회귀 분석을 생각해 봅시다. 단순성형회귀 분석에서 x의 정보는 '집의크기' 였습니다. 이때 이 '집의크기'는 하나의 ``feature``라고 합니다. 다중선형회귀 분석에서 집값을 결정하는 요인으로 집의 크기, 집의 층수, 집이 지어진 연도, 역과의 거리 4개를 사용한다고 하면 feature의 개수는 총 4개 입니다.

``sample``은 x와 y 한 세트를 의미합니다. 첫번째 집의 정보가 $$ X_1 = (x_{1,집의크기}, x_{1,집의 층수}, x_{1,집이 지어진 연도}, x_{1,역과의 거리})$$이고 가격 $$ Y_1 = {y_1} $$ 로 주어지면 이것을 한 sample이라고 합니다. 이런 식으로 총 몇개의 집이 주어지는지에 따라 sample의 개수가 결정되겠죠.  

## Matrix

아래 그림을 봅시다 좌표의 개수는 (1,2), (2,4), (4,3) (5,4) 총 4개의 `sample`이 있습니다. 또한 집값을 결정하는 x 요인은 집의크기 하나이므로 feature의 개수는 1개 입니다.

![graph]({{ site.url }}/img/regression-graph.PNG)

이 단순선형 회귀를 가지고 실제 문제를 풀어보겠습니다. 이 데이터로부터 x와 y의 관계를 유추하고 집 크기에 따른 집값을 예측하고 싶습니다. 이 x와 y의 관계를 유추하기 위한 식을 세우는데 이 때 머신러닝에서는 이 식을 가설(Hypothesis)라고 합니다. 아래의 H(x)에서 H는 Hypothesis를 의미합니다.

* H(x) = Wx + b

W, b의 값에 따라서 좌표평면에는 다양한 직선이 그려지게 됩니다. 그 직선들 중에 데이터를 가장 잘 설명하는 직선을 얻는 것이 우리의 목표입니다. 그렇다면 어떻게 적절한 W와 b의 값을 찾을 수 잇을까요?

## Cost function: MSE
식을 통해 얻은 예측값 $$\hat{y}$$ 과 실제값 y 의 오차가 적을 수록 적절한 W와 b를 찾았다 할 수 있겠죠. 이 때 실제값과 예측값의 오차에 대한 식을 목적 함수(Objective function), 비용함수(Cost function) 또는 손실함수(Loss function) 라고 합니다. 

손실함수는 단순히 실제값과 예측값에 대한 오차를 표현할 뿐만이 아니라 예측값의 오차를 줄이는 일에 최적화 된 식이여야 합니다. 손실함수의 종류에는 각 문제에 적합한 함수들이 있지만 회귀분석에서는 주로 평균 제곱 오차(Mean Squred Error, MSE)를 사용합니다. 
> $$ cost(W,b) = 1/N \sum (y_i-H(x_i))^2 $$

![graph]({{ site.url }}/img/regression-graph2.PNG)

위에 그래프처럼 임의의 직선을 그렸을 때 MSE 식에서 볼수 있듯이 각 점들이 직선과 떨어진 거리의 제곱, 즉 그림에서 사각형의 모양을 합한 것입니다. 따라서 MSE는 저 사각형들의 크기의 평균을 나타낸다 할 수 있습니다. 사각형의 크기를 줄이는 것은 오차를 줄이는 것과 동일합니다. 즉 cost function cost(W,b)를 최소가 되게 만드는 W와 b를 구하면 결과적으로 y 와 x의 관계를 가장 잘 나타내는 직선을 그릴 수 있게 됩니다.

## Optimizer: Gradient Descent

이제 앞서 정의한 비용함수의 값을 최소로 하는 W와 b를 찾는 방법에 대해서 배울



