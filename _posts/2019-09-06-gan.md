---
layout: post
title: Generative Adversarial Network, GAN
excerpt: "GAN의 개념을 이해하고 구현해보자"
categories: [GAN practice]
comments: true
---

2014년 Ian Goodfellow가 NPS에서 발표한 paper에서 시작한 GAN은 2016년에 특히 크게 떠올랐습니다. 이번 시간에는 GAN에 대한 개념을 소개하고 케라스로 직접 코드를 구현해 보도록 하겠습니다. 본 포스팅은 [Jaejun Yoo's Playground](http://jaejunyoo.blogspot.com/2017/01/generative-adversarial-nets-2.html) 블로그 글을 참고한 것임을 밝힘니다.

>  [Jaejun Yoo's Playground](http://jaejunyoo.blogspot.com/2017/01/generative-adversarial-nets-2.html)

## GAN 개념적 소개

Generative Adversarial Network 라는 이름에서 알 수 있듯이 우선 ``Adversrial``는 ``대립``, ``적대``의 뜻을 지닙니다. GAN은 이렇게 서로 대립하는 두 부분으로 나누어져 있다는것을 짐작할 수 있습니다. 
GAN은 생성하는 ``Generator``와  Generator 가 만든 녀석을 평가하는 ``Discriminaotr``가 있고 서로 대립 (Adversarial) 하며 서로의 성능을 점차 개선해나갑니다. Ian Goofellow가 논문에서 이를 설명한 것이 유명하기도 하고 재미있기 때문에 소개합니다. 

> 지폐위조범(Generator)은 경찰을 최대한 열심히 속이려고 하고 다른 한편에서는 경찰(Discriminator)이 이렇게 위조된 지폐를 진짜와 감별하려고(Classify) 노력한다.
이런 경쟁 속에서 두 그룹 모두 속이고 구별하는 서로의 능력이 발전하게 되고 결과적으로는 진짜 지폐와 위조 지폐를 구별할 수 없을 정도(구별할 확률 $$p_d=0.5$$)에 이른다는 것.

이제 이 예시를 좀 더 구체적으로 얘기하면 다음과 같습니다. Generator G는 우리가 가지고 있는 data x의 distribution을 알아내려고 노력합니다. 만약 G가 정확히 data distribution을 모사할 수 있다면 그렇게 만들어진 sample은 data와 구별할 수 없을 정도로 같겠죠.
한편 discrimianor D는 현재 자기가 보고 있는 sample이 training data에서 온 것인지 G에서 만들어진 것인지 구별하여 각각 경우에 대한 확률을 측정합니다. 

![gan-3]({{ site.url }}/img/gan-3.PNG)

위 그림에서 볼 수 있듯이 최종 아웃풋은 D(s) 입니다. 이 때 s는 ``real data x``와 generator 에서 만들어진 ``G(z)`` 중 랜덤하게 골라진 sample 입니다. (s 라는 표기는 이해를 돕기 위해 제가 임의로 사용했고 실제 논문에는 사용하지 않았습니다.)

D의 입장에서는 만약 sample s 가 ``real data x`` 라면 ``D(x)=1``이 되도록 노력할 것이고 sample s 가 generator가 생성한 ``fake data G(z)`` 라면 ``D(G(z))=0`` 이 되도록 노력 할 것입니다. 즉, D는 실수할 확률을 낮추기 위해 (mini) 노력하고 반대로 G는 D가 실수할 확률을 높이기 (max) 위해 노력하는데, 따라서 둘을 같이 놓고 보면 **"minimax tow-player game of minimax problem"** 이라 할 수 있겠습니다. 

## Adversarial Nets

* Input noise varibales $$p_z(z)$$
* Generator's distribution over data x $$p_g$$
* Real data distribution $$p_{data(x)}$$

``Generator G``는 real data x 의 distribution을 학습하기 위해 input noise variable $$p_z(z)$$에 대한 prior을 정의하고 fake data를 만들어내기에 $$G(z;\theta_g)$$로 표현 할 수 있습니다. ``G(z)``는 미분 가능한 함수로써 $$\theta_g$$를 parameter로 갖는 multilayer perceptron 입니다.

``Discriminator D`` 역시 multilyaer perceptron으로 $$D(s;\theta_d)$$로 나타내며 output은 확률이므로 signle scalar 값으로 나타내게 됩니다. 따라서 ``D(s)``는 s가 generator's distirbution $$p_g$$가 아닌 real data distribution $$p_x$$ 로부터 왔을 확률을 나타냅니다.

따라서 이를 수식으로 정리하면 다음과 같은 value function V(G,D)에 대한 minmax problem을 푸는 것과 같아집니다. 

![gan-6]({{ site.url }}/img/gan-6.PNG) 

극단적인 예시를 들어 수식에 넣어보면 이 수식을 한결 이해하기 편합니다. 먼저 가장 이상적인 상황에서의 D를 생각해 봅니다. D는 아주 잘 구별을 하는 함수입니다. 그렇기에 D가보는 sample s가 ``real data distriubtion`` $$p_{data}$$ 에서 온 것이라면, ``D(x) = 1``이므로 첫번째 term은 0이 됩니다. 반대로 D가보는 sample s 가 ``Generator distribution`` $$p_{g}$$ 에서 온 것이라면, ``D(G(z)) = 0``이므로 두번재 term 역시 0으로 사라집니다. 따라서 이 때 D의 입장에서 V의 최대값을 얻을 수 있다는 것은 자명합니다. 

## 학습과정

![gan-2]({{ site.url }}/img/gan-2.PNG)

> 논문(NIPS 2014)

이 그림은 위에 설명한 내용을 이해하기 좋게 잘 그려놓았습니다. 먼저 검정색 점선이 ``data distribution`` $$p_x$$, 파랑색 점선이 ``discriminator `` ``distribution``, 녹색 선이 ``generative distribution`` $$p_g$$ 입니다. 밑에 선은 각각 x와 z의 domain을 나타내며, 위로 뻗은 화살표가 $$x=G(z)$$의 mapping을 보여줍니다. 

처음 시작할 때는 ``(a)``와 같이 $$p_g$$(초록색 선)와 $$p_{data}$$(검정색 점선)의 distribution이 전혀 다르게 생긴 것을 볼 수 있습니다. 이 상태에서 discriminator D가 두 distribution을 구별하기 위해 학습을 하면 ``(b)``와 같이 좀 더 smooth하고 잘 구별하는 **disriminator distribution**(파랑색 점선)이 먼저 만들어집니다. 이후 G가 D가 구별하기 어려운 방향으로 학습을 하면 ``(c)``와 같이 $$p_g$$가 $$p_{data}$$와 가까워지게 되고 이런식으로 학습을 반복하다 보면 결국 ``(d)`` 그림과 같이 $$p_g = p_{data}$$가 되어 discriminator가 둘을 전혀 구별하지 못하는 $$D(x) = \frac{1}{2}$$ 상태가 됩니다.  

## 케라스 구현

Tensor flow 2.0 beta 이전 version ( 1.9.x )을  backend로 사용한 코드입니다. 실습 코드는 Smart Design Lab의 튜토리얼을 참고하였습니다.
{: .notice}

 > [Smart Design Lab](http://www.smartdesignlab.org/dl.html)
 
### Library import 

{% highlight python %} 
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
{% endhighlight %}

{% highlight python %} 
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("./mnist/data/", one_hot=True)
{% endhighlight %}

### Hyper parameters
{% highlight python %} 
total_epoch = 30
batch_size = 100
learning_rate = 0.0002
{% endhighlight %}

### Cell option parameters
{% highlight python %} 
n_hidden = 256
n_input = 28*28
n_noise = 128
{% endhighlight %}

{% highlight python %} 
X = tf.placeholder(tf.float32, [None, n_input]) 
Z = tf.placeholder(tf.float32, [None, n_noise])
{% endhighlight %}

### Generator
* Hidden layer는 ``relu`` 사용하며 ``noise_z``가 input으로 들어갑니다.
* Output은 ``sigmoid``를 사용하여 0과 1사이 값으로 output이 나오도록 합니다.

{% highlight python %} 
G_W1 = tf.Variable(tf.random_normal([n_noise, n_hidden], stddev=0.01))
G_b1 = tf.Variable(tf.zeros([n_hidden])) 

G_W2 = tf.Variable(tf.random_normal([n_hidden, n_input], stddev=0.01))
G_b2 = tf.Variable(tf.zeros([n_input]))

def generator(noise_z):
    hidden = tf.nn.relu(tf.matmul(noise_z, G_W1) + G_b1) 
    output = tf.nn.sigmoid(tf.matmul(hidden, G_W2) + G_b2)
    return output
  
G_var_list = [G_W1, G_b1, G_W2, G_b2]
{% endhighlight %}

### Discriminator

* Distriminator의 최종 결과값은 input data가 real data일 확률을 나타내는 한 개의 스칼라값으로 나타납니다.
* 첫번째 layer에서 ``relu``를 사용하고 output은 ``sigmoid``를 이용하여 0 에서 1 사이 값을 출력합니다.

{% highlight python %} 
D_W1 = tf.Variable(tf.random_normal([n_input, n_hidden], stddev=0.01))
D_b1 = tf.Variable(tf.zeros([n_hidden]))


D_W2 = tf.Variable(tf.random_normal([n_hidden, 1], stddev=0.01))
D_b2 = tf.Variable(tf.zeros([1])) 

def discriminator(inputs):
    hidden = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1) 
    output = tf.nn.sigmoid(tf.matmul(hidden, D_W2) + D_b2)
    return output
  
  
D_var_list = [D_W1, D_b1, D_W2, D_b2]
{% endhighlight %}

### Noise
* 랜덤 노이즈 ``Z``를 생성하고 이를 이용해 랜덤한 이미지를 생성합니다.
* 생성된 이미지를 판별할 값을 실제 이미지를 이용해 구합니다.

{% highlight python %} 
def get_noise(batch_size, n_noise):
    return np.random.normal(size=(batch_size, n_noise))
G = generator(Z)

D_gene = discriminator(G)
D_real = discriminator(X)
{% endhighlight %}

### Genorator loss
* 생성기(G)는 가짜 이미지를 넣었을 때 D가 실제 이미지라고 판단하도록 D_gene을 극대화 해야 합니다.
* GAN 논문의 수식에 따르면 loss를 극대화 해야하지만, minimize 하는 최적화 함수를 사용하기 때문에 loss_G에 음수 부호를 붙여줍니다.

{% highlight python %} 
loss_G = tf.reduce_mean(tf.log(D_gene))
train_G = tf.train.AdamOptimizer(learning_rate).minimize(-loss_G,var_list=G_var_list)
{% endhighlight %}

### Discriminator loss
* 판별기(D)에 진짜 이미지 ``tf.log(D_real)``를 넣었을 때도, 가짜 이미지 ``tf.log(1 - D_gene)``를 넣었을 때도 최대값을 갖도록 학습합니다.
* GAN 논문의 수식에 따르면 loss를 극대화 해야하지만, ``minimize`` 하는 최적화 함수를 사용하기 때문에 ``loss_G``에 ``음수`` 부호를 붙여줍니다.

{% highlight python %} 
loss_D = tf.reduce_mean(tf.log(D_real) + tf.log(1 - D_gene))
train_D = tf.train.AdamOptimizer(learning_rate).minimize(-loss_D,var_list=D_var_list)
{% endhighlight %}

### Model training

{% highlight python %} 
# 신경망 모델 학습

sess = tf.Session() # 세션 생성
sess.run(tf.global_variables_initializer()) # 변수 초기화

total_batch = int(mnist.train.num_examples/batch_size) # 전체 이미지 갯수/batch_size 해서 한번 학습 할때 100batch_size로 몇번을 해야 1epoch을 도는지 계산
loss_val_D, loss_val_G = 0, 0 # 변수 0으로 초기화

for epoch in range(total_epoch):
    for i in range(total_batch):
        batch_xs, batch_ys = mnist.train.next_batch(batch_size)
        noise = get_noise(batch_size, n_noise)

        # 판별기와 생성기 신경망을 각각 학습
        _, loss_val_D = sess.run([train_D, loss_D], feed_dict={X: batch_xs, Z: noise})
        _, loss_val_G = sess.run([train_G, loss_G], feed_dict={Z: noise})

    print('Epoch:', '%04d' % epoch,
          'D loss: {:.4}'.format(loss_val_D),
          'G loss: {:.4}'.format(loss_val_G))
    
    
    # 학습이 되어가는 모습을 보기 위해 주기적으로 이미지를 보여줌
    if epoch == 0 or (epoch + 1) % 10 == 0:
        sample_size = 10
        noise = get_noise(sample_size, n_noise)
        samples = sess.run(G, feed_dict={Z: noise})
        
        fig, ax = plt.subplots(1, sample_size, figsize=(sample_size, 1))

        for i in range(sample_size):
            ax[i].imshow(np.reshape(samples[i], (28, 28)), 'gray')
        
        plt.show()

print('최적화 완료!')
{% endhighlight %}

### Training results

![gan-4]({{ site.url }}/img/gan-4.PNG)

![gan-5]({{ site.url }}/img/gan-5.PNG)
