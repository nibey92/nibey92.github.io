---
layout: post
title: Long Short-Term Memory
excerpt: "LSTM의 작동원리"
categories: [deep learning]
comments: true
---

이번 포스팅은 여러 블로그 글들을 참고하였습니다. 주로 참고한 블로그 글은 다음과 같습니다.
{: .notice}

 > [The Missing Papers](http://docs.likejazz.com/lstm/)
 
 > [ratsgo's blog](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)
 
 
## LSTM의 기본 구조
다음 그림은 Chris Olah의 LSTM 구조 이미지를 재해석하여 그렸습니다. 
![lstm]({{ site.url }}/img/lstm.png)

LSTM이 RNN과 다른 큰 특징은 `cell state` 라고 불리는 `c`입니다. LSTM에는 이 `cell state`를 보호하고 컨트롤 하기 위한 세가지 `gate` 가 존재합니다. `forget gate`, `input gate`, 그리고 `output gate` 입니다. 이 gate들을 통해 vanishing gradient 를 방지하고 gradient가 효과적으로 흐를 수 있게 합니다. 지금부터 각 gate에 대해서 하나씩 자세히 알아보도록 하겠습니다.

### forget gate 
과거 정보를 잊기 위한 gate 입니다. 아래 그림에서 보다시피, $$h_{t-1}$$와 $$x_t$$ 를 인수로 받아 sigmoid 함수를 취합니다. 이 값의 출력 범위는 0 에서 1 사이이기 때문에 그 값이 0이라면 이전 상태의 정보는 잊고, 1이라면 이전상태의 정보를 온전히 기억하게 됩니다. $$f_t$$ 의 식은 다음과 같습니다. 
* $$f_{t} = \sigma (W_{f}*x_{t} +U_{f}*{h_{t-1}} + b_{f} )$$

![lstm_f]({{ site.url }}/img/lstm_f.png)

### input gate
현재 정보를 기억하기 위한 gate 입니다. 아래 그림에서처럼 h_t-1과 x_t를 받아 sigmoid 함수를 취한 값 $$i_t$$와 hyperbolic tangent를 취한 값  $$g_t$$를 hadamard product 연산을 합니다. $$i_t$$ 의 값은 0과 1사이의 값, $$g_t$$의 값은 -1 에서 1 사이의 값이 나오기 때문에 input gate의 결과값은 음수가 나올 수도 있습니다. $$i_t$$ 와 $$g_t$$ 의 식은 다음과 같습니다.
* $$i_{t} = \sigma (W_{i}*x_{t} +U_{i}*{h_{t-1}} + b_{i} )  $$
* $$g_{t} = tanh (W_{o}*x_{t} +U_{o}*{h_{t-1}} + b_{o} )  $$

![lstm_i]({{ site.url }}/img/lstm_i.png)

### ouput gate
최종 결과 $$h_t$$ 를 위한 gate 입니다. $$o_t$$ 연산 자체는 $$f_t$$나 $$i_t$$ 처럼 sigmoid 함수를 사용합니다.
* $$o_{t} = \sigma (W_{o}*x_{t} +U_{o}*{h_{t-1}} + b_{o} )  $$

![lstm_o]({{ site.url }}/img/lstm_o.png)

### cell state and hidden state value. ( $$c_t$$, $$h_t$$ )
$$c_t$$값을 구할 때는 앞에서 얻은 `forget gate` 와 `input gate`의 값 그리고 `이전 cell state` 값인 $$c_{t-1}$$가 필요합니다. $$c_{t-1}$$과 $$f_t$$ 를 hadamard product 한 값과 input gate에서 얻은 값 ($$i_t \circ g_t$$)을 더해준 값이 $$c_t$$가 됩니다. 이 $$c_t$$에 hyperbolic tangent를 취해주고 output gate 값과 hadamard product해주면 $$h_t$$를 얻을 수 있습니다. $$c_t$$와 $$h_t$$의 식은 다음과 같습니다. 
* $$c_t = f_t \circ c_{t-1} + i_t \circ g_t $$
* $$h_t = o_t \circ \sigma (c_t) $$

![lstm_ch]({{ site.url }}/img/lstm_ch.png)


## LSTM의 순전파와 역전파

### 순전파
### 역전파

## 코드 구현
코드로 구현하기 전 앞에서 사용한 수식들을 정리해 보겠습니다.

* $$f_{t} = \sigma (W_{f}*x_{t} +U_{f}*{h_{t-1}} + b_{f} )$$
* $$i_{t} = \sigma (W_{i}*x_{t} +U_{i}*{h_{t-1}} + b_{i} )  $$
* $$g_{t} = tanh (W_{o}*x_{t} +U_{o}*{h_{t-1}} + b_{o} )  $$
* $$o_{t} = \sigma (W_{o}*x_{t} +U_{o}*{h_{t-1}} + b_{o} )  $$
* $$c_t = f_t \circ c_{t-1} + i_t \circ g_t $$
* $$h_t = o_t \circ \sigma (c_t) $$

이제 수식들을 코드로 구현해봅니다.
{% highlight ruby %}
ft = sigmoid(np.dot(xt, Wf) + np.dot(ht_1, Uf) + bf)  # forget gate
it = sigmoid(np.dot(xt, Wi) + np.dot(ht_1, Ui) + bi)  # input gate
gt = np.tanh(np.dot(xt, Wg) + np.dot(ht_1, Ug) + bg)  # input gate
ot = sigmoid(np.dot(xt, Wo) + np.dot(ht_1, Uo) + bo)  # output gate
Ct = ft * Ct_1 + it * gt
ht = ot * np.tanh(Ct)

ht_1 = ht  # hidden state, previous memory state
Ct_1 = Ct  # cell state, previous carry state
{% endhighlight %}


## Keras 구현
Keras에서 LSTM을 구현하는 코드는 매우 간단하고 직관적입니다. 백엔드로는 TensorFlow를 사용하였습니다. 현재 연구중인 data cluster 분류에서 실제로 LSTM을 사용하고 있습니다. 물론 실제 사용할 때는 이보다 더 복잡한 LSTM 모델을 만들어 사용하지만 여기서는 아주 간단한 LSTM 모델을 구현해보겠습니다. 

{% highlight ruby %}
model = Sequential()
model.add(LSTM(32, input_shape=(25,9))
model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])
{% endhighlight %}

[The Missing Papers](http://docs.likejazz.com/lstm/) 님의 글을 읽다가 알게 된건데, Keras에서는 sigmoid가 아닌 Hard Sigmoid를 사용한다고 합니다. 저도 Keras를 이용해 LSTM을 구현하고 있었는데 몰랐던 사실이였습니다. 

{% highlight ruby %}
def hard_sigmoid(x):
    return np.clip(0.2 * x + 0.5, 0, 1)
{% endhighlight %}

따라서 Keras를 이용해 구현한 값과 NumPy를 이용해 구현한 값을 비교하기 위해서는 처음 사용한 sigmoid 값을 hard_sigmoid로 바꿔줘야 할 필요가 있습니다. 저는 LSTM의 구조에 대한 이해를 목적으로 포스팅 하였기 때문에 이 이상은 진행하지 않기로 했습니다. 혹시 결과까지 알고 싶은 분은 [The Missing Papers](http://docs.likejazz.com/lstm/) 님의 블로그에서 확인하시면 됩니다.

