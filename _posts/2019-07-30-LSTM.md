---
layout: post
title: Long Short-Term Memory
excerpt: "LSTM의 작동원리"
categories: [deep learning]
comments: true
---

이번 포스팅은 여러 블로그 글들을 참고하였습니다. 주로 참고한 블로그 글은 다음과 같습니다.
{: .notice}

 > [The Missing Papers](http://docs.likejazz.com/lstm/)
 
## LSTM의 기본 구조
LSTM을 설명하는 Chris Olah 글 이미지를 재해석하여 그렸습니다. 
![lstm]({{ site.url }}/img/lstm.png)

LSTM이 RNN과 다른 큰 특징은 `cell state C` 값이 존재한다는 것인데요, 이 cell state를 보호하고 컨트롤 하기 위한 세가지 gate 가 존재합니다.
`forget gate`, `input gate`, 그리고 `output gate` 입니다. 이 세가지 gate를 통해 vanishing gradient 를 방지하고 gradient가 효과적으로 흐를 수 있게 합니다. 세가지 gate에서는 sigmoid 함수를 사용하고 있습니다

![lstm_f]({{ site.url }}/img/lstm_f.png)
* forget gate: 과거 정보를 잊기 위한 게이트 입니다. sigmoid 함수의 출력 범위는 0~1 이기 때문에 그 값이 0이라면 이전 상태의 정보는 잊고, 1이라면 이전상태의 정보를 온전히 기억하게 됩니다.

![lstm_i]({{ site.url }}/img/lstm_i.png)
* input gate: 현재 정보를 기억하기 위한 게이트 입니다. 그림으로 설명하면 ht-1과 xt 를 받아 시그모이드 취한 i 와 똑같은 방법으로 하이퍼볼릭 탄젠트를 취한 Ct 를 hdamard product 연산 한 값입니다. i의 값은 0~1 사이, gt 범위는 -1~1 이기 때문에 결과는 음수가 될 수도 있습니다. 

![lstm_o]({{ site.url }}/img/lstm_o.png)
* ouput gate: 최종 결과  ht 를 위한 게이트입니다. Ct의 하이퍼볼릭 탄젠트를 hadamard product한 결과가 ht가 됩니다.

