---
layout: post
title: Generative Adversarial Network, GAN
excerpt: "GAN의 개념을 이해하고 구현해보자"
categories: [GAN practice]
comments: true
---

2014년 Ian Goodfellow가 NPS에서 발표한 paper에서 시작한 GAN은 2016년에 특히 크게 떠올랐습니다. 

## GAN 개념적 소개

Generative Adversarial Network 라는 이름에서 알 수 있듯이 우선 ``Adversrial``는 ``대립``, ``적대``의 뜻을 지닙니다. GAN은 이렇게 서로 대립하는 두 부분으로 나누어져 있다는것을 짐작할 수 있습니다. 
GAN은 생성하는 Generator와  Generator 가 만든 녀석을 평가하는 Discriminaotr가 있고 서로 대립 (Adversarial) 하며 서로의 성능을 점차 개선해나갑니다. Ian Goofellow가 논문에서 이를 설명한 것이 유명하기도 하고 재미있기 때문에 소개합니다. 

> 지폐위조범(Generator)은 경찰을 최대한 열심히 속이려고 하고 다른 한편에서는 경찰(Discriminator)이 이렇게 위조된 지폐를 진짜와 감별하려고(Classify) 노력한다.
이런 경쟁 속에서 두 그룹 모두 속이고 구별하는 서로의 능력이 발전하게 되고 결과적으로는 진짜 지폐와 위조 지폐를 구별할 수 없을 정도(구별할 확률 $$p_d=0.5$$)에 이른다는 것.

이제 이 예시를 좀 더 구체적으로 얘기하면 다음과 같습니다. Generator G는 우리가 가지고 있는 data x의 distribution을 알아내려고 노력합니다. 만약 G가 정확히 data distribution을 모사할 수 있다면 그렇게 만들어진 sample은 data와 구별할 수 없을 정도로 같겠죠.
한편 discrimianor D는 현재 자기가 보고 있는 sample이 training data에서 온 것인지 G에서 만들어진 것인지 구별하여 각각 경우에 대한 확률을 측정합니다. 

![gan-1]({{ site.url }}/img/gan-1.PNG)

일단 G와 Drk 둘다 multilayer perceptron model을 사용하면
Generator's distribution p_g over data x 를 학습하기 위해 generator 의 input으로 들어갈 noise variables pz(z)에 대한 prior를 정의하고, data space로의 mapping을 G(z;\theta_g) 라 표현할 수 있습니다. 여기서 G는 미분 가능한 함수로써 $$\theta_g$$를 parameter로 갖는 multilayer perceptron입니다.

input noise variables p_z(z)
real data distribution p_data(x)

한편, Discriminator 역시 multilyaer perceptron으로 D(s;\theta_d)로 나타내며 output은 확률이므로 signle scalar 값으로 나타내게 됩니다. 따라서 D(s)는 s가 generator's distirbution p_z가 아닌 real data distribution p_x 로부터 왔을 확률을 나타냅니다.

따라서  D(s=G(z)) 라면 Discriminaotr 입장에서는 0에 가까운 값을 나타내도록 해야하고 
D(s=x) (sample이 real data에서 왔을 경우) 라면 1에 가까운 값을 나타내도록 해야 겠죠.

따라서 이를 수식으로 정리하면 다음과 같은 value function V(G,D)에 대한 minmax problem을 푸는 것과 같아집니다. 

$$ min max V(D,G) = E_{x~P_{data}(x)}[logD(x)] + E_{z~P_{z}(x)}[1-D(G(z)))]$$ 
