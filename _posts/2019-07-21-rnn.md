---
layout: post
title: Recurent Neural Network
excerpt: "RNN의 구조에 대해 알아보자"
categories: [deep learning]
comments: true
---

이번 포스팅은 여러 블로그 글들을 참고하였습니다. 주로 참고한 블로그 글은 다음과 같습니다.
{: .notice}

 > [ratsgo's blog](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)
 
 > [쉽게 읽는 프로그래밍](https://m.blog.naver.com/PostView.nhn?blogId=magnking&logNo=221311273459&proxyReferer=https%3A%2F%2Fwww.google.com%2F)


## RNN 이란

Recurrent Neural Network가 지닌 가장 큰 특징은 데이터의 순서와 시간을 고려할 수 있다는 것입니다. 덕분에 RNN 은 나오자마자 sequntial data를 이용하는 분야에서 많은 사랑을 받았는데요, 특히 문장 번역이나 음성 분석, 주식과 같은 연속적이며(sequential) 시계열(time series) 특징을 지닌 데이터에 많이 사용되었습니다. 이번 포스팅에서는 ``Recurrent``

## RNN의 기본구조

일반적인 신경망을 Feed-forward neural networks(FFNets) 라고 하는데요, FFNets에서는 데이터를 입력하면 데이터가 각 노드를 한번씩 지나가며 연산이 진행되고 출력값이 나오게 됩니다. 각 노드를 한번만 지나가게 된다는 것은 데이터의 순서, 시간 이란 측면을 고려하지 않는다고 할 수 있습니다. 

![RNN_2]({{ site.url }}/img/rnn_v2.PNG)

하지만 RNN의 경우는 hidden layer의 결과가 다시 같은 hidden layer 의 입력으로 들어가도록 연결되어 있습니다. 이때 이 hidden layer의 값을 $$h_t$$ 라고 합니다. 아래 그림에서처럼 RNN은 기존의 NN와 달리 input value `x`, output value `y` 외에 또 하나의 파라미터 ``hidden state value h``가 있습니다. 현재 상태의 hidden state 값인 h_t는 직전 시점의 hidden state h_t-1을 받아 갱신됩니다. 바로 이 ``h`` 가 RNN으로 하여금 이전 state 에서의 정보를 기억하도록 하는 요소라 할 수 있습니다. 그렇다면 어떤 계산으로 h가 나오는지 hidden layer 를 자세히 알아보도록 합시다.



## Hidden layer의 작동원리

![HiddenLayer]({{ site.url }}/img/rnn_hidenlayer.PNG)

그림에서 보시는 것처럼 hidden layer 안에서는 두개의 함수가 작동하고 있습니다. 하나는 $$x_t$$ 와 $$h_{t-1}$$ 를 input 으로 받아 $$h_t$$ 를 얻는 함수이고 다른 하나는 이렇게 얻어진 $$h_t$$를 받아 $$y_t$$를 도출하는 함수입니다. 두 식을 나타내면 다음과 같습니다.
* $$h_{t} = tanh( W_{xh} * x_{t} + W_{hh} * h_{t-1} + b )$$ 
* $$y_{t} = W_{hy} * h_{t} + b$$

가중치 W 는 총 세개입니다. W_xh, W_hh, W_hy. 따라서 RNN이 학습하는 parametersms 이 세개의 가중치입니다. 그리고 모든 시점의 statae에서 이 parameter는 동일하게 적용됩니다. (shaered weights.)

## RNN의 순전파

## RNN의 역전파
