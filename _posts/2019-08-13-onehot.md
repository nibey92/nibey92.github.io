---
layout: post
title: One-hot encoding
excerpt: "One-hot encoding에 대해 알아보고 간단한 코드로 구현해 봅니다."
categories: [preprocessing]
comments: true
---

컴퓨터는 문자보다는 숫자를 더 잘 인식합니다. 특히 문자를 많이 다루는 자연어처리에서 문자를 숫자로 바꾸는 기법을 사용 하는데 One-hot encoding은 가장 기본적인 기법이며 머신러닝, 딥러닝에서 자주 사용하는 방법입니다. 시작하기 앞서 이번 글은 [딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/22647) 의 `원-핫 인코딩` 글을 공부하고 그 내용을 제 나름대로 풀어 쓴 것임을 밝힙니다.

## One-hot encoding
원-핫 인코딩을 위해서 먼저 해야할 일은 갖고 있는 우선 단어 집합을 만드는 일입니다. 텍스트의 모든 단어를 중복을 허락하지 않고 모아놓는다면 이를 `단어 집합(Vocabulary)`이라고 합니다. 단어 집합은 자연어처리에서 계속해서 등장하는 개념입니다. 단어 집합은 한마디로 서로 다른 단어들의 집합입니다. 예를들어 사과, 바나나, 딸기의 집합이 있다면 단어 집합의 크기는 3 입니다. 단어 집합에서는 book과 books와 같이 단어의 변형 형태도 다른 단어로 간주합니다.

그 다음으로는 이 단어 집합의 단어들마다 0번부터 2번까지의 인덱스를 부여합니다. 사과는 0번, 바나나는 1번, 딸기는 2번과 같이 부여됩니다. 이러한 과정을 `정수인코딩(Integer encoding)`이라고 합니다. 이렇게 단어들 모두에 대해서 인덱스를 부여하고 나면 각 단어에 해당하는 인덱스에 1의 값을 부여하고 다른 인덱스에는 0을 부여하여 단어를 벡터로 표현할 수 있습니다. 이렇게 표현된 벡터를 원-핫 벡터(One-hot vector)라고 합니다. 아래의 그림을 참고하면 이해하기 쉽습니다.

## Keras로 구현하기
케라스에서는 자동으로 원-핫 인코딩을 만들어주는 to_categorical()를 지원합니다. 케라스만으로 정수 인코딩과 원-핫 인코딩을 순차적으로 진행해보도록 하겠습니다.

### 정수 인코딩
{% highlight ruby %} 
text="나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야"
{% endhighlight %}

위와 같은 문장이 있다고 했을 때, 먼저 정수 인코딩을 위해 케라스틔 Tokenizer를 이용합니다.
{% highlight ruby %} 
from keras_preprocessing.text import Tokenizer

text="나랑 점심 먹으러 갈래 점심 메뉴는 햄버거 갈래 갈래 햄버거 최고야"

t = Tokenizer()
t.fit_on_texts([text])
# 입력으로 [text]가 아닌 text를 넣을 경우 한 글자 단위 인코딩이 되버립니다. ex 갈 : 1, 래 : 2

print(t.word_index) # 각 단어에 대한 인코딩 결과 출력.
{% endhighlight %}
실행 결과는 다음과 같습니다.
{% highlight ruby %} 
{'갈래': 1, '점심': 2, '햄버거': 3, '나랑': 4, '먹으러': 5, '메뉴는': 6, '최고야': 7}
{% endhighlight %}

이와 같이 생성된 단어 집합(Vocabulary)에 있는 단어들로만 구성된 텍스트가 있다면, texts_to_sequences()를 통해서 이를 바로 인덱스의 나열로 변환가능합니다. 생성된 단어 집합의 단어들로만 구성된 서브 텍스트 text2를 가지고 확인해보겠습니다.

{% highlight ruby %} 
text2="점심 먹으러 갈래 메뉴는 햄버거 최고야"

x=t.texts_to_sequences([text2])[0]
#결과가 리스트의 리스트로 나오는 것이 불편하면 [0]을 써줍니다.
print(x)
{% endhighlight %}

text2의 정수 인코딩 결과는 다음과 같습니다.
{% highlight ruby %} 
[2, 5, 1, 6, 3, 7]
{% endhighlight %}

### 원-핫 인코딩 
지금까지 진행한 것은 이미 정수 인코딩 챕터에서 배운 정수 인코딩 과정입니다. 이제 해당 결과를 가지고, 원-핫 인코딩을 진행해보도록 하겠습니다. 케라스는 정수 인코딩 된 결과를 입력으로 받아 바로 원-핫 인코딩 과정을 수행하는 to_categorical()를 지원합니다.

{% highlight ruby %} 
from keras.utils import to_categorical

vocab_size = len(t.word_index) # 단어 집합의 크기. 이 경우는 단어의 개수가 7이므로 7.
x = to_categorical(x, num_classes=vocab_size+1) # 실제 단어 집합의 크기보다 +1로 크기를 만들어야함.

print(x)
{% endhighlight %}
아래 결과는 "점심 먹으러 갈래 메뉴는 햄버거 최고야"라는 문장이 [2, 5, 1, 6, 3, 7]로 정수 인코딩이 되고나서, 각각의 인코딩 된 결과를 인덱스로 원-핫 인코딩이 수행된 모습을 보여줍니다.
{% highlight ruby %} 
[[0. 0. 1. 0. 0. 0. 0. 0.] #인덱스 2의 원-핫 벡터
 [0. 0. 0. 0. 0. 1. 0. 0.] #인덱스 5의 원-핫 벡터
 [0. 1. 0. 0. 0. 0. 0. 0.] #인덱스 1의 원-핫 벡터
 [0. 0. 0. 0. 0. 0. 1. 0.] #인덱스 6의 원-핫 벡터
 [0. 0. 0. 1. 0. 0. 0. 0.] #인덱스 3의 원-핫 벡터
 [0. 0. 0. 0. 0. 0. 0. 1.]] #인덱스 7의 원-핫 벡터
{% endhighlight %}

여기서 주의할 점은 to_categorical()은 정수 인코딩으로 부여된 인덱스를 그대로 배열의 인덱스로 사용하기 때문에, t.fit_on_texts()를 사용하여 정수 인코딩을 하였을 경우에는 실제 단어 집합의 크기보다 +1의 크기를 인자로 주어야 한다는 점입니다. t.fit_on_texts()는 인덱스를 1부터 부여합니다. 하지만 배열의 인덱스는 0부터 시작하므로 맨 마지막 인덱스를 가진 단어의 인덱스가 7인데, 이를 원-핫 벡터로 만들기 위해서는 총 8의 크기를 가진 배열이 필요합니다.

## One-hot encoding의 한계

이러한 표현 방식은 단어의 개수가 늘어날 수록, 벡터를 저장하기 위해 필요한 공간이 계속 늘어난다는 단점이 있습니다. 다른 말로는 벡터의 차원이 계속 늘어난다고도 표현합니다. 원 핫 벡터는 단어 집합의 크기가 곧 벡터의 차원 수가 됩니다. 가령, 단어가 1,000개인 코퍼스를 가지고 원 핫 벡터를 만들면, 모든 단어 각각은 모두 1,000개의 차원을 가진 벡터가 됩니다. 다시 말해 모든 단어 각각은 하나의 값만 1을 가지고, 999개의 값은 0의 값을 가지는 벡터가 되는데, 이는 매우 비효율적인 표현 방법입니다.

또한 이런 방식은 단어의 유사성을 전혀 표현하지 못한다는 단점이 있습니다. 예를 들어서 늑대, 호랑이, 강아지, 고양이라는 4개의 단어에 대해서 원-핫 인코딩을 해서 각각, [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]이라는 원-핫 벡터를 부여받았다고 합시다. 이 때 이 원-핫 벡터 표현 방법을 통해서는 강아지와 늑대가 유사하고, 호랑이와 고양이가 유사하다는 것을 표현할 수가 없습니다. 좀 더 극단적인 예를 들어보겠습니다. 원-핫 벡터로는 유사성을 표현할 수 없기 때문에, 강아지와 개라는 단어 유사 정도나 강아지와 냉장고라는 뜬금없는 유사 관계나 차이를 알아볼 수가 없습니다.

단어 간 유사성을 표현, 계산할 수 없다는 단점은 검색 시스템 등에서 심각한 문제입니다. 가령, 여행을 가려고 웹 검색창에 '삿포로 숙소'라는 단어를 검색한다고 합시다. 제대로 된 검색 시스템이라면, '삿포로 숙소'라는 검색어에 대해서 '삿포로 게스트 하우스', '삿포로 료칸', '삿포로 호텔'과 같은 유사 단어에 대한 결과도 함께 보여줄 수 있어야 합니다. 하지만 단어간 유사성을 계산할 수 없다면, '게스트 하우스'와 '료칸'과 '호텔'이라는 연관 검색어를 보여줄 수 없습니다.

이러한 단점을 없앨 수 있는, 단어의 '의미'를 다차원 공간에 벡터화 하는 기법으로는 두 가지가 있습니다. 첫째는 카운트 기반으로 단어의 의미를 벡터화하는 LSA, HAL 등이 있으며, 둘째는 예측 기반으로 단어의 의미를 벡터화하는 전통 NNLM, RNNLM, Word2Vec, FastText 등이 있습니다. 그리고 카운트 기반과 예측 기반 두 가지 방법을 모두 사용하는 방법으로 Glove라는 방법이 존재합니다.

