---
layout: post
title: Back Propagation
excerpt: "역전파 이해하기"
categories: [deep learning]
comments: true
---

주로 참고한 블로그 글은 다음과 같습니다.
{: .notice}
 
 > [딥 러닝을 이용한 자연어 처리 입문](https://wikidocs.net/37406)

## Neural Network Overview
예제를 위해 사용될 인공 신경망은 앞에서 소개한 사이트의 모델과 같은 것을 사용합니다. 이 인공 신경망은 입력층, 은닉층, 출력층 3개의 층을 가집니다. 또한 각 층에는 두개의 뉴런이 있습니다. 은닉층과 출력층의 모든 뉴런은 시그모이드 함수를 활성화 함수로 사용합니다.
![propa]({{ site.url }}/img/propa.PNG)

#### 은닉층
* W : 입력층에서 은닉층 방향으로 향하는 가중치 입니다.
* z : 입력층의 x 값에 가중치 W가 곱해진 값입니다.
* h : z 값이 시그모이드 함수를 지난 후 값으로 은닉층의 최종 출력값입니다.

#### 출력층
* U : 은닉층에서 출력층 방향으로 향하는 가중치 입니다.
* t : 은닉층의 출력값 h에 가중치 U가 곱해진 값입니다.
* o : t 값이 시그모이드 함수를 지난 후의 값으로 출력층의 최종 출력값입니다.
 
이번 역전파 예제에서는 인공 신경망에 존재하는 모든 가중치 W와 U에 대해서 역전파를 통해 업데이트하는 것을 목표로 합니다. 해당인공 신경망은 편향 b는 고려하지 않습니다.

## Forward Propagation
![propa2]({{ site.url }}/img/propa2.PNG)
역전파를 계산하기 전에 먼저 순전파를 진행하여 딥러닝이 학습하는 순서를 먼저 알아봅시다. x, W, U, 그리고 실제값 y는 모두 상수로 주어집니다. 그림에서 모든 값들을 표시하였습니다. 이 값들을 코드에 먼저 초기값으로 적어놓습니다.

{% highlight ruby %} 
import numpy as np

x1= 0.1 
x2= 0.2

W11 = 0.3 
W21 = 0.25
W12 = 0.4 
W22 = 0.35

U11 = 0.45
U21 = 0.4
U12 = 0.7
U22 = 0.6

y1 = 0.4
y2 = 0.6

{% endhighlight %}

이제 차례대로 각 층에서의 값들을 계산해봅니다. 
#### 은닉층
* $$ z_1 = W_{11}*x_1 + W_{21}*x_2 = 0.08 $$
* $$ z_2 = W_{12}*x_1 + W_{22}*x_2 = 0.11 $$
* $$ h_1 = sigmoid(z_1) = 0.51998934 $$
* $$ h_2 = sigmoid(z_2) = 0.0.52747230 $$

#### 출력층
* $$ t_1 = U_{11}*h_1 + U_{21}*t_2 = 0.44498412 $$
* $$ t_2 = U_{12}*h_1 + U_{22}*t_2 = 0.68047592 $$
* $$ o_1 = sigmoid(t_1) = 0.60944600 $$
* $$ o_2 = sigmoid(t_2) = 0.66384491 $$

#### 오차
이제 해야할 일은 예측값과 실제값의 오차를 계산하기 위한 오차함수를 선택하는 것입니다. 오차를 계산하기 위한 손실함수는 평균제곱오차(Mean squared error, MSE)를 사용합니다. 각 오차를 모두 더하면 전체 오차가 됩니다. 
> $$ MSE = 1/N \sum (y_i-\hat{y})^2 $$

* $$ E_1 = \frac{1}{2}(y_1-o_1)^2 = 0.02193381 $$
* $$ E_2 = \frac{1}{2}(y_2-o_2)^2 = 0.00203809 $$
* $$ E_{tot} = E_1 + E_2 = 0.02397190 $$

코드로 구현하여 계산을 해보겠습니다. 순전파의 경우 간단한 곱셈과 시그모이드 함수로 값을 얻을 수 있기 때문에 쉽게 구현이 가능합니다.

{% highlight ruby %} 
def sigmoid(x):
  return 1/(1+np.exp(-x))

z1 = x1*W11+x2*W21
z2 = x1*W12+x2*W22
h1 = sigmoid(z1)
h2 = sigmoid(z2)

t1 = h1*U11+h2*U21
t2 = h1*U12+h2*U22
o1 = sigmoid(t1)
o2 = sigmoid(t2)

E1=0.5*(y1-o1)**2
E2=0.5*(y2-o2)**2
E=E1+E2

{% endhighlight %}

코드를 실행시켜 결과를 비교해 봅니다. 
{% highlight ruby %} 

------hidden layer------
z1: 0.08
z2: 0.11
h1: 0.519989340156
h2: 0.527472304345
------ out layer--------
t1: 0.444984124808
t2: 0.680475920716
o1: 0.609446004997
o2: 0.663844909698
------ Entrophy---------
E1: 0.0219338145045
E2: 0.00203808624719
E: 0.0239719007517
------------------------

{% endhighlight %}

## Back Propagation Step 1
순전파가 입력층에서 출력층으로 향한다면 역전파는 반대로 출력층에서 입력층 방향으로 계산하면서 가중치를 업데이트해 갑니다. 먼저 출력층과 은닉층 사이의 가중치 ``U`` 를 업데이트하는 단계를 ``역전파 1단계``, 은닉층과 입력층 사이의 가중치 ``W``를 업데이트 하는 단계를 ``역전파 2단계``라고 합시다. 

우선 역전파 1단계를 진행하겠습니다. 역전파 1단계에서 업데이트해야 할 가중치는 $$U_{11}, U_{21}, U_{12}, U_{22}$$ 총 4개입니다. 네개의 가중치를 업데이트하는 원리는 같기 때문에 먼저 $$U_{11}$$ 에 대해서 계산하겠습니다. $$U_{11}$$를 업데이트 하기 위해 경사하강법을 수행하려면 우리는 $$ \frac{\partial E_{tot}}{\partial U_{11}}$$을 계산해야 합니다. 그런데 이 값을 바로 계산 하기는 힘들기 때문에 ``chain rule``을 사용합니다. Chain rule 식을 쓰기가 헷갈리신다면 아래 그림을 참고하시면 됩니다. 그림에서처럼 $$E_{tot}$$와 $$U_{11}$$ 사이에 존재하는 $$o_1$$과 $$t_1$$ 값을 가지고 chain rule 식을 쓰면 됩니다. 그럼 다음과 같이 풀어 쓸수 있습니다.

* $$ \frac{\partial E_{tot}}{\partial U_{11} } = 
\frac{\partial E_{tot}}{\partial o_1} \times 
\frac{\partial o_1 }{\partial t_1} \times 
\frac{\partial t_1}{\partial U_{11} } $$

이렇게 chain rule로 풀어쓰면 우변의 세 항들을 쉽게 계산할수 있습니다. 세 항들을 차례대로 계산해 봅시다.
#### 첫째항
$$ \frac{\partial E_{tot}}{\partial o_1} $$

$$ \centerdot E_{tot} = E_1 + E_2 = \frac{1}{2}(y_1-o_1)^2 + \frac{1}{2}(y_2-o_2)^2 $$ 

$$ \therefore \frac{\partial E_{tot}}{\partial o_1} = -(y_1-o_1) $$

#### 둘째항
sigmoid 함수의 미분값은 $$ f(x) \times (1-f(x)) $$ 입니다. sigmoid 함수의 미분은 기억해 주는것이 좋습니다. 이를 이용하여 두번재 항의 미분을 계산하면 다음과 같습니다.
* $$ \frac{\partial o_1}{\partial t_1} $$

$$ o_1 = sigmoid(t_1) $$ 

$$ \frac{\partial o_1}{\partial t_1} = o_1*(1-o_1) $$

#### 셋째항
* $$ \frac{\partial t_1}{\partial U_{11} } $$

$$ t_1 = U_{11} * h_1 + U_{21} * t_2 $$

$$ \frac{\partial t_1}{\partial U_{11} } = h_1 $$

우변의 모든 항을 계산 하였습니다. 이제 이 값을 모두 곱해주면 됩니다. 최종값의 모든 파라미터들은 상수기 때문에 값만 넣어서 계산하면 됩니다. 계산 결과는 다음과 같습니다.

* $$ \frac{\partial E_{tot}}{\partial U_{11} } = 
\frac{\partial E_{tot}}{\partial o_1} \times 
\frac{\partial o_1 }{\partial t_1} \times 
\frac{\partial t_1}{\partial U_{11} } $$  

$$=-(y_1-o_1) \times o_1(1-o_1) \times h_1 $$

$$= 0.02592286 $$

이제 앞에서 배웠던 경사하강법을 통해 가중치를 업데이트 하면 됩니다. 하이퍼파라미터에 해당하는 learning rate $$\alpha$$는 0.5라고 가정합니다. 그러면 업데이트되는 가중치 $$U_{11}^{+}$$는
* $$U_{11}^{+} = U_{11} - \alpha * \frac{\partial E_{tot}}{\partial U_{11} } $$

$$= 0.45 - 0.5 * 0.02592286 $$

이와 같은 원리로 $$ U_{11}, U_{21}, U_{12}, U_{22} $$ 를 계산할 수 있습니다.

## Back Propagation Step 2
