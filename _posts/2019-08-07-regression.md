---
layout: post
title: Regression
excerpt: "Linear regression"
categories: [machine learning]
comments: true
---

이 포스팅은 `KMOOC 자율주행을위한 머신러닝 강좌`를 청강한 후 정리하였습니다.
{: .notice}

딥러닝을 이해하기 위해서는 Linear Regression과 Logistic Regression을 이해할 필요가 있습니다. 오늘은 우선 선형회귀에 대해서 알아보겠습니다. 선형회귀 개념 자체에 대한 이해도 중요하지만 머신러닝에서 쓰이는 용어인 가설, 손실함수, 경사하강법에 대한 개념까지 함께 이해하도록 하겠습니다.

## Linear regression

시험 공부 시간이 늘어날수록 성적이 잘 나오고 운동시간을 늘릴수록 몸무게는 줄어듭니다. 집의 평수가 클수록 집의 매매 가격은 비싼 경향이 있습니다. 수학적으로 생각해보면 어떤 요인의 수치가 특정 수치에 영향을 주고있습니다. 다른 변수의 값을 변하게 하는 변수를 ``x``, 변수 x에 의해 값이 종속적으로 변하는 변수를 ``y``라고 합니다. 

선형 회귀는 종속 변수 y와 한 개 이상의 독립 변수 x 와의 선형 관계를 모델링하는 분석 기법입니다. 즉 x는 1개일수도, 그 이상일 수도 있습니다. 독립변수 x가 1개일 때 단순 선형 회귀라고 합니다.


### Simple Linear Regression Analysis

집의 크기에 따른 집값을 생각 해 봅시다. 집의 크기를 x, 집값이 y라 하면 수식은 다음과 같이 나타낼 수 있습니다.

* $$y = Wx + b$$ 

위의 수식은 단순선형회귀(Simple Linear Regression Analysis)의 수식을 보여줍니다. 여기서 W를 ``가중치(weight)``, b를 ``편향(bias)`` 이라고 합니다.

### Multiple Linear Regression Analysis

* $$ y = W_1x_1 + W_2x_2 + .... W_nx_n +b $$

잘 생각해보면 집값은 집의 크기 뿐만이 아니라 집의 층수, 집이 지어진 연도, 역과의 거리 등의 요소들도 영향을 미칩니다. 이렇게 다수의 요소를 가지고 집의 가격을 예측해 봅시다. y는 여전히 1개이지만 x는 이제 여러개가 되었습니다. 이를 다중 선형 회귀 분석(Multiple Linear Regression Analysis)이라고 합니다.

### feature, sample

위에서 보았던 단순선형회귀 분석과 다중선형회귀 분석을 생각해 봅시다. 단순성형회귀 분석에서 x의 정보는 '집의크기' 였습니다. 이때 이 '집의크기'는 하나의 ``feature``라고 합니다. 다중선형회귀 분석에서 집값을 결정하는 요인으로 집의 크기, 집의 층수, 집이 지어진 연도, 역과의 거리 4개를 사용한다고 하면 feature의 개수는 총 4개 입니다.

``sample``은 x와 y 한 세트를 의미합니다. 첫번째 집의 정보가 $$ X_1 = (x_{1,집의크기}, x_{1,집의 층수}, x_{1,집이 지어진 연도}, x_{1,역과의 거리})$$이고 가격 $$ Y_1 = {y_1} $$ 로 주어지면 이것을 한 sample이라고 합니다. 이런 식으로 총 몇개의 집이 주어지는지에 따라 sample의 개수가 결정되겠죠.  

## Hypothesis

아래 그림을 봅시다 좌표의 개수는 (1,2), (2,4), (4,3) (5,4) 총 4개의 `sample`이 있습니다. 또한 집값을 결정하는 x 요인은 집의크기 하나이므로 feature의 개수는 1개 입니다.

![graph]({{ site.url }}/img/regression-graph.PNG)

이 단순선형 회귀를 가지고 실제 문제를 풀어보겠습니다. 이 데이터로부터 x와 y의 관계를 유추하고 집 크기에 따른 집값을 예측하고 싶습니다. 이 x와 y의 관계를 유추하기 위한 식을 세우는데 이 때 머신러닝에서는 이 식을 가설(Hypothesis)라고 합니다. 아래의 H(x)에서 H는 Hypothesis를 의미합니다.

* H(x) = Wx + b

W, b의 값에 따라서 좌표평면에는 다양한 직선이 그려지게 됩니다. 그 직선들 중에 데이터를 가장 잘 설명하는 직선을 얻는 것이 우리의 목표입니다. 그렇다면 어떻게 적절한 W와 b의 값을 찾을 수 잇을까요?

## Cost function: MSE
식을 통해 얻은 예측값 $$\hat{y}$$ 과 실제값 y 의 오차가 적을 수록 적절한 W와 b를 찾았다 할 수 있겠죠. 이 때 실제값과 예측값의 오차에 대한 식을 목적 함수(Objective function), 비용함수(Cost function) 또는 손실함수(Loss function) 라고 합니다. 

비용함수는 단순히 실제값과 예측값에 대한 오차를 표현할 뿐만이 아니라 예측값의 오차를 줄이는 일에 최적화 된 식이여야 합니다. 손실함수의 종류에는 각 문제에 적합한 함수들이 있지만 회귀분석에서는 주로 평균 제곱 오차(Mean Squred Error, MSE)를 사용합니다. 
> $$ cost(W,b) = 1/N \sum (y_i-H(x_i))^2 $$

이 식이 뜻하는 것은 무엇일까요? 아래 그림처럼 특정한 값 W, b를 정하여 직선을 그렸을 때 각 점들이 직선과 떨어진 거리의 제곱, 즉 사각형들을 모두 더한 것입니다. 이 사각형들은 오차를 시각화 한 것입니다. 결국 cost(W,b)가 뜻하는것은 이 사각형들의 크기의 평균입니다. 

![graph]({{ site.url }}/img/regression-graph2.PNG)

이 사각형들의 크기를 줄이는 것, 즉 cost(W,b)를 최소가 되게 만드는 W와 b를 구하면 결과적으로 y 와 x의 관계를 가장 잘 나타내는 직선을 그릴 수 있게 됩니다. 비용함수를 통해서 오차를 얻었으니 이제 이 오차를 줄이는 방법을 알아 보도록 합시다.

## Optimizer: Gradient Descent Algorithm

선형 회귀를 포함한 수많은 머신러닝, 딥러닝의 학습은 결국 비용 함수를 최소화하는 매개 변수인 W와 b를 찾기 위한 작업을 수행합니다. 이때 사용되는 것이 Optimizer 알고리즘입니다. 최적화 알고리즘이라고도 부릅니다. 그리고 이 옵티마이저 알고리즘을 통해 적절한 W와 b를 찾아내는 과정을 학습(training)이라고 합니다. 여기서는 가장 기본적인 옵티마이저 알고리즘인 경사 하강법(Gradient Descent)를 가지고 문제를 풀어보도록 하겠습니다.

cost(W)와 W의 관계는 이차함수의 꼴을 가지고 있습니다. 따라서 기울기 W가 무한대로 커지면 커질수록 cost값 또한 무한대로 커지고 반대로 W가 무한대로 작아져도 cost의 값은 무한대로 커집니다. 위의 그래프에서 cost가 가장 작을 때는 맨 아래의 볼록한 부분입니다. 기계가 해야할 일은 cost가 가장 최소값을 가지게 하는 W를 찾는 일이므로 이차함수 맨아래 볼록한 부분의 W의 값을 찾아야 합니다.

![graph]({{ site.url }}/img/regression-graph5.PNG)

경사하강법의 아이디어는 랜덤값 W 값을 정한뒤에 맨 아래의 볼록한 부분을 향해 점차 W 값을 업데이트하는 것입니다. 맨 아래의 볼록한 부분은 미분을 배웠더라면 접선의 기울기가 0이 되는 지점이라는 것을 쉽게 알 수 있습니다. 이를 바탕으로 cost를 최소화 하는 W 값을 찾는 경사 하강법의 식은 다음과 같습니다.

* $$ W^+ = W - \alpha \frac{\partial}{\partial W} cost(W) $$

여기서 $$\alpha$$는 ``학습률(learning rate)``라고 합니다. 학습률은 우리가 임의로 적당한 값을 줍니다. 우선 $$\alpha$$를 생각하지 않고 현재 가중치 W에서 W를 미분한 값을 빼는것이 어떤 의미를 지니는지 알아보겠습니다.

![graph]({{ site.url }}/img/regression-graph3.PNG)

위의 그림은 접선의 기울기가 음수일 때, 0일때, 양수일 때의 경우를 보여줍니다. 기울기가 음수면 경사 하강법의 식에서 W의 값이 증가하게 되는데 이는 결과적으로 접선의 기울기가 0인 방향으로 W 값이 조정됩니다. 만약 접선의 기울기가 양수라면 W의 값이 감소하게 되는데 이는 결과적으로 기울기가 0인 방향으로 W의 값이 조정됩니다. 즉 cost를 최소화하는 식은 접선의 기울기가 음수이거나 양수일 때 모두 접선의 기울기가 0인 방향으로 W의 값을 조정합니다.

그렇다면 학습률 $$\alpha$$는 어떤 의미를 가질까요? $$\alpha$$는 W 값을 변경할 때 얼마나 크게 변경할지를 결정합니다. 또는 W를 그래프의 한 점으로 보고 접선의 기울기가 0일 때까지 경사를 따라 내려간다는 관점에서는 얼마나 큰 폭으로 이동할 것인지 결정합니다. 만약 $$\alpha$$의 값을 무작정 크게 한다면 어떻게 될까요?

![graph]({{ site.url }}/img/regression-graph4.PNG)

위의 그림은 학습률 $$\alpha$$가 지나치게 높은 값을 가질 때 접선의 기울기가 0이 되는 W를 찾아가는 것이 아니라 W 값이 발산하는 상황을 보여줍니다. 반대로 학습률 $$\alpha$$가 지나치게 낮으면 학습속도가 느려지므로 적당한 $$\alpha$$를 찾아내는 것도 중요합니다. 이번 예시에서는 b를 배제하고 W값을 찾아내는 것에 초점을 맞췄지만 실제 경사 하강법은 W와 b에 대해 동시에 경사하강법을 수행합니다. 이 방법은 여러 분야에서 다양하게 쓰였던 아주 기본적인 방법이라 할 수 있습니다. 물리학에서는 물리학에서는 stepeest decrease algorithm 이라고도 합니다.
